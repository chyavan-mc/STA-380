# 1. Probability practice

## Part. A
Visitors to your website are asked to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories: Random Clicker (RC), and Truthful Clicker (TC). There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the information that the expected fraction of random clickers is 0.3.  After a trial period, you get the following survey results: 65\% said Yes and 35\% said No.   What fraction of people who are truthful clickers answered yes?  Hint: use the rule of total probability.

> Lets start with the mathematical representations of all the fractions we know, <br>
> $P(Y) = 0.65 \rightarrow$ Marginal probability of a clicker saying Yes <br>
> $P(N) = 0.35 \rightarrow$ Marginal probability of a clicker saying No <br>
> $P(R_{c}) = 0.3 \rightarrow$ Probability of a clicker being a random clicker <br>
> $P(T_{c}) = 0.7 \rightarrow$ Probability of a clicker being a truthful clicker <br>
> 
> To find the fraction of people truthful clickers who answered yes, we find the value of $P(Y|T_{c})$
> 
> We know that, <br>
>   $P(Y) = P(Y|R_{c}) + P(Y|T_{c})$
> 
> Rearranging, <br>
>   $P(Y|T_{c}) = P(Y) - P(Y|R_{c})$
> 
> If a random clicker clicking *Yes* or *No* is equally likely, then the probability (or fraction) of Random clickers, clicking yes > is (Fraction of users among the 0.3 random clickers, clicking *Yes*), <br>
>   $P(Y|R_{c}) = 0.5*0.3 = 0.15$
> 
> Substituting in the rearranged equation, <br>
>   $P(Y|T_{c}) = 0.65 - 0.15$ <br>
>   $P(Y|T_{c}) = 0.5$
>
> Therefore, the fraction of clickers among truthful clickers who clicked yes would be **0.5**

## Part. B
Imagine a medical test for a disease with the following two attributes:  

- The sensitivity is about 0.993. That is, if someone has the disease, there is a probability of 0.993 that they will test positive.
- The specificity is about 0.9999. This means that if someone doesn't have the disease, there is probability of 0.9999 that they will test negative.
- In the general population, incidence of the disease is reasonably rare: about 0.0025% of all people have it (or 0.000025 as a decimal probability).

Suppose someone tests positive. What is the probability that they have the disease?

> Lets start with the mathematical representations of all the probabilities we know, <br>
>   $P(T_{p}|D) = 0.993 \rightarrow$ Probability of testing positive given a person has the disease <br>
>   $P(T_{n}|D_{c}) = 0.9999 \rightarrow$ Probability of testing negative given a person doesn't have the disease <br>
>   $P(D) = 0.000025 \rightarrow$ Probability of a person having the disease <br>
>   $P(D_{c}) = 1- P(D) = 0.999975$ \rightarrow Probability of a person not having the disease <br>
> 
> We'll find the probability of testing positive, given a person doesn't have the disease, i.e.
>   $P(T_{p}|D_{c}) = 1 - P(T_{n}|D_{c}) = 1 - 0.9999 = 0.0001$
> 
> Probability of having the disease given that someone tested positive <br>
>   $P(D|T_{p}) = \frac{P(T_{p}|D)*P(D)}{P(T_{p})}$ <br>
>   $P(D|T_{p}) = \frac{P(T_{p}|D)*P(D)}{P(T_{p}|D)*P(D) + P(T_{p}|D_{c})*P(D_{c})}$ <br>
>   $P(D|T_{p}) = \frac{0.993*0.000025}{0.993*0.000025 + 0.0001*0.999975}$ = 0.1989<br>
>   $P(D|T_{p}) = 0.1989 \rightarrow 19.89\%$ <br>
>
> Therefore, the probability of having the disease given that someone tested positive is **0.1989 or 19.89%**. It has significantly increased from the marginal probability of having the disease which was **0.000025 or 0.0025%**

# 2. Wrangling the Billboard Top 100

using [billboard.csv](./Data/billboard.csv)
``` {r 2setup, include=FALSE}
library(dplyr)
library(tidyverse)

options(dplyr.summarise.inform = FALSE)
```

```{r 2pre}
billboard = read.csv("./Data/billboard.csv")
billboard = billboard[,c("performer", "song", "year", "week", "week_position")]
# Considering the relevant columns as mentioned in the question
head(billboard)
```

## Part. A

Make a table of the top 10 most popular songs since 1958, as measured by the _total number of weeks that a song spent on the Billboard Top 100._  Note that these data end in week 22 of 2021, so the most popular songs of 2021 will not have up-to-the-minute data; please send our apologies to The Weeknd.    

Your table should have __10 rows__ and __3 columns__: `performer`, `song`, and `count`, where `count` represents the number of weeks that song appeared in the Billboard Top 100.  Make sure the entries are sorted in descending order of the `count` variable, so that the more popular songs appear at the top of the table.  Give your table a short caption describing what is shown in the table.  

(_Note_: you'll want to use both `performer` and `song` in any `group_by` operations, to account for the fact that multiple unique songs can share the same title.)  

``` {r 2a, warnings=FALSE}
top_songs_by_week = billboard %>% group_by(song, performer) %>% summarize(count = n())
top10_songs_by_week = head(top_songs_by_week[order(-top_songs_by_week$count),], 10)

knitr::kable(top10_songs_by_week,
             caption = "The Top 10 Songs (Jul 1958 - Jun 2021) by the number of weeks on the Billboard")
```

## Part. B

Is the "musical diversity" of the Billboard Top 100 changing over time?  Let's find out.  We'll measure the musical diversity of given year as _the number of unique songs that appeared in the Billboard Top 100 that year._  Make a line graph that plots this measure of musical diversity over the years.  The x axis should show the year, while the y axis should show the number of unique songs appearing at any position on the Billboard Top 100 chart in any week that year.  For this part, please filter the data set so that it excludes the years 1958 and 2021, since we do not have complete data on either of those years.   Give the figure an informative caption in which you explain what is shown in the figure and comment on any interesting trends you see.

There are number of ways to accomplish the data wrangling here.  We offer you two hints on two possibilities:  

1) You could use two distinct sets of data-wrangling steps.  The first set of steps would get you a table that counts the number of times that a given song appears on the Top 100 in a given year.  The second set of steps operate on the result of the first set of steps; it would count the number of unique songs that appeared on the Top 100 in each year, _irrespective of how many times_ it had appeared.
2) You could use a single set of data-wrangling steps that combines the `length` and `unique` commands. 

``` {r 2b, warnings=FALSE}
musical_diversity = billboard[,c("performer", "song", "year")] 
musical_diversity = musical_diversity %>% filter(!(year %in% c(2021, 1958)))
musical_diversity = unique(musical_diversity)
musical_diversity = musical_diversity %>% group_by(year) %>% summarize(unique=n())
musical_diversity = musical_diversity[order(musical_diversity$year),]

# plot(musical_diversity$year, musical_diversity$unique, type="l", col="red", xlab="Year", ylab="# of unique songs on the Billboard Top-100", main="Musical Diversity")

ggplot(data=musical_diversity, aes(x=year, y=unique, group=1)) +
  geom_line(color="red", size=1) +
  ggtitle("Musical Diversity") +
  xlab("Year") +
  ylab("# of unique songs on the Billboard Top-100")
```

## Part. C

Let's define a "ten-week hit" as a single song that appeared on the Billboard Top 100 for at least ten weeks.  There are 19 artists in U.S. musical history since 1958 who have had _at least 30 songs_ that were "ten-week hits."  Make a bar plot for these 19 artists, showing how many ten-week hits each one had in their musical career.   Give the plot an informative caption in which you explain what is shown.


_Notes_:  

1) You might find this easier to accomplish in two distinct sets of data wrangling steps.
2) Make sure that the individuals names of the artists are readable in your plot, and that they're not all jumbled together.  If you find that your plot isn't readable with vertical bars, you can add a `coord_flip()` layer to your plot to make the bars (and labels) run horizontally instead.
3) By default a bar plot will order the artists in alphabetical order.  This is acceptable to turn in.  But if you'd like to order them according to some other variable, you can use the `fct_reorder` function, described in [this blog post](https://datavizpyr.com/re-ordering-bars-in-barplot-in-r/).  This is optional.

``` {r 2c}
ten_week_hitters = top_songs_by_week %>%
  filter(count >= 10) %>%
  group_by(performer) %>%
  summarize(count = n()) %>%
  filter(count >= 30)

ggplot(data=ten_week_hitters, aes(x=reorder(performer, +count), y=count)) +
  geom_bar(stat="identity", fill="steelblue") +
  ylab("# of 10-week-hits") + xlab("Artist") +
  coord_flip()
```


# 3. Visual story telling part 1: Green Buildings

using [greenbuildings.csv](./Data/greenbuildings.csv) <br>
*\*use raw preview to view the R-code*

``` {r 3, include=FALSE}
library(dplyr)
library(tidyverse)

green_db = read.csv("./Data/greenbuildings.csv")

green_buildings = green_db[green_db$green_rating == 1,]
non_green_buildings = green_db[green_db$green_rating == 0,]
```

``` {r 3a, echo=FALSE}
cat(dim(green_db[green_db$leasing_rate < 0.1, ])[1], "outliers were removed from the data")
```
160 is a significant number of outliers to be discarded with a justification of data cleaning. Let us also see how the outliers modified the median value of the rent
``` {r 3b, echo=FALSE}
cat(
  "Median rent of all green buildings:", median(green_db[green_db$green_rating == 1, "Rent"]),
  "\nMedian rent of all green buildings after removing the outliers:", median(green_db[green_db$green_rating == 1 & green_db$leasing_rate > 0.1, "Rent"])
  )
```
The outlier removal does not seem to have affected the median rate at all, rendering the process completely redundant. Additionally, removing the outliers might have actually cost us a lot in terms of information loss. Let's see how
``` {r 3c, echo=FALSE}
obs_per_cluster = green_db %>% group_by(cluster) %>% summarise(count=n())
removed_per_cluster = green_db[green_db$leasing_rate < 0.1, ] %>% group_by(cluster) %>% summarise(count=n())

merged = merge(obs_per_cluster, removed_per_cluster, by="cluster", suffixes = c(".total",".removed"))
merged$perc_removed = 100*merged$count.removed/merged$count.total
merged = merged[order(-merged$perc_removed),]

plot_merge = merged[merged$perc_removed >= 20, c("cluster", "perc_removed")]
rownames(plot_merge) = NULL
knitr::kable(plot_merge,
             align = "ll",
             col.names = c("cluster", "perc_removed"),
             caption = "The Percentage of cluster rows lost because of the outlier removal (top 27)")

cat("In these 27 clusters, at least 20% of their properties (rows) were removed when the outliers were discarded. From intuition, we may suppose that each cluster might have characteristic rent of its own. So discarding a significant proportion of each cluster is removing vital information. We can also see that the outlier removal removed ",
      dim(green_db[green_db$leasing_rate < 0.1 & green_db$green_rating == 1, ])[1],
     " of the green rated property.")

```

Let's see how the data is cluster-wise. Let us group the clusters and look at the increase in rent for that cluster for a green building, and then summarize it.

``` {r 3d, echo=FALSE, warning=FALSE}
non_green_medians = non_green_buildings %>% group_by(cluster) %>% summarize(median_rent = median(Rent))
merged = merge(green_db, non_green_medians, by="cluster", suffixes=c(".all", ".non_green"))
comparison = merged[merged$green_rating == 1, c("cluster", "Rent","median_rent")]
comparison$difference = comparison$Rent - comparison$median_rent

cat("Median increase in rent for a green property (among all cluster-wise increases): $", median(comparison$difference), sep=)

ggplot(comparison, aes(x=difference)) + 
  geom_histogram(binwidth=3, color="darkblue", fill="lightblue") +
  geom_vline(xintercept = median(comparison$difference), linetype="dashed",color = "red", size=1) +
  geom_text(aes(x=-3+median(comparison$difference), label="Median Rent Increase", y=35), colour="red", angle=90, text=element_text(size=11)) +
  xlab("Rent difference for green buildings") + ylab("Number of clusers")
most_sign_differences = data.frame(table(cut(comparison$difference, 30)))
most_sign_differences = most_sign_differences[order(-most_sign_differences$Freq),]
rownames(most_sign_differences) = NULL
knitr::kable(
  head(most_sign_differences, 3))
```

From the histogram, we can see that the top 3 most frequent buckets (range of rent increases) are as shown in the table above. Among the top-3, 2 are negative, implying the rents for green buildings are lower than the non-green buildings in some of the clusters. That's all aggregated data. Now let's see how the spread of the rent difference is among clusters.

``` {r 3e, echo=FALSE, warning=FALSE}
iqr = quantile(comparison$difference,probs=c(0.25,0.75))
knitr::kable(iqr)
```

the mid-50% of the data is in the range of (-0.4125,6.3475) meaning we can expect our rent to increase by this amount (per sqaure feet) if we ignore all other factors that might effect the rent. What other factors might be having an effect on the rent?

``` {r 3f, echo=FALSE, warning=FALSE, fig.width=8, fig.height=8}
par(mfrow=c(2,2))
plot(green_db$leasing_rate, green_db$Rent, ylab="Rent", xlab="Leasing Rate")
plot(green_db$Electricity_Costs, green_db$Rent, ylab="Rent", xlab="Electricity cost")
plot(green_db$cd_total_07, green_db$Rent, ylab="Rent", xlab="Cooling  Demand")
plot(green_db$hd_total07, green_db$Rent, ylab="Rent", xlab="Heating  Demand")
```

We can see, although not clearly, that the rent varies with leasing rate, electricity cost, cooling demand, heating demand, etc. all of which depend on the location of the property. This information about our propery under consideration being on East Cesar Chavez, just across I-35 from downtown is never being considered in the analysis. Considering these variations might also give us a more *reliable* prediction of the rent difference.

One more factor considered in the analysis was 100% or 90% occupancy from day-1 of the property being ready for occupancy. This is rarely the case. Let us assume the median rent difference of $3 per square fit per year. and let us consider the property being occupied 100% over a span of some years

``` {r 3g, echo=FALSE}
y3 = c(.33, .66, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)
y5 = c(.2, .4, .6, .8, 1.0, 1.0, 1.0, 1.0)
y8 = c(.125, .25, .375, .5, .625, .75, .875, 1.0)
annual_income_full_occupancy = 250000*3

year3 = c()
year5 = c()
year8 = c()

for(i in 1:8){
  year3 = c(year3, annual_income_full_occupancy*y3[i])
  year5 = c(year5, annual_income_full_occupancy*y5[i])
  year8 = c(year8, annual_income_full_occupancy*y8[i])
}


target_occupancy_100 = c(
  round(3 + ((5000000-sum(year3[1:3]))/annual_income_full_occupancy), 2),
  round(5 + ((5000000-sum(year5[1:5]))/annual_income_full_occupancy), 2),
  round(8 + ((5000000-sum(year8[1:8]))/annual_income_full_occupancy), 2)
)

y3 = y3*0.75
y5 = y3*0.75
y8 = y3*0.75

year3 = c()
year5 = c()
year8 = c()

for(i in 1:8){
  year3 = c(year3, annual_income_full_occupancy*y3[i])
  year5 = c(year5, annual_income_full_occupancy*y5[i])
  year8 = c(year8, annual_income_full_occupancy*y8[i])
}


target_occupancy_75 = c(
  round(3 + ((5000000-sum(year3[1:3]))/(annual_income_full_occupancy*0.75)), 2),
  round(5 + ((5000000-sum(year5[1:5]))/(annual_income_full_occupancy*0.75)), 2),
  round(8 + ((5000000-sum(year8[1:8]))/(annual_income_full_occupancy*0.75)), 2)
)

df = data.frame(target_occupancy_100, target_occupancy_75)
rownames(df) = c("Target occupancy reached by year 3", "Target occupancy reached by year 5", "Target occupancy reached by year 8")
knitr::kable(df,
             align = "ll",
             caption = "The Number of years required to recuperate the premium for green certification")

```

With this analysis, 7.7 years is still the best case scenario of recuperating the premium spent on a green certification. For a more realistic scenario, we can see that the cost will be recuperated before making any profits after at least 10 years of the construction. This gives a more realistic picture by just considering the rent increase of the median cluster disregarding all other factors. We can still suggest the developer to go for a linear regression model to get a more accurate prediction of the returns. Additionally, the linear model would also give a sense the role a particular factor plays in the estimation of the returns.


# 4. Visual story telling part 2: Capital Metro data

using [capmetro_UT.csv](./Data/capmetro_UT.csv) <br>
*\*use raw preview to view the R-code*