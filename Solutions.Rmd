---
output:
  md_document:
    variant: markdown_github
---

# 1. Probability practice

## Part. A
Visitors to your website are asked to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories: Random Clicker (RC), and Truthful Clicker (TC). There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the information that the expected fraction of random clickers is 0.3.  After a trial period, you get the following survey results: 65\% said Yes and 35\% said No.   What fraction of people who are truthful clickers answered yes?  Hint: use the rule of total probability.

> Lets start with the mathematical representations of all the fractions we know, <br>
> $P(Y) = 0.65 \rightarrow$ Marginal probability of a clicker saying Yes <br>
> $P(N) = 0.35 \rightarrow$ Marginal probability of a clicker saying No <br>
> $P(R_{c}) = 0.3 \rightarrow$ Probability of a clicker being a random clicker <br>
> $P(T_{c}) = 0.7 \rightarrow$ Probability of a clicker being a truthful clicker <br>
> 
> To find the fraction of people truthful clickers who answered yes, we find the value of $P(Y|T_{c})$
> 
> We know that, <br>
>   $P(Y) = P(Y|R_{c}) + P(Y|T_{c})$
> 
> Rearranging, <br>
>   $P(Y|T_{c}) = P(Y) - P(Y|R_{c})$
> 
> If a random clicker clicking *Yes* or *No* is equally likely, then the probability (or fraction) of Random clickers, clicking yes > is (Fraction of users among the 0.3 random clickers, clicking *Yes*), <br>
>   $P(Y|R_{c}) = 0.5*0.3 = 0.15$
> 
> Substituting in the rearranged equation, <br>
>   $P(Y|T_{c}) = 0.65 - 0.15$ <br>
>   $P(Y|T_{c}) = 0.5$
>
> Therefore, the fraction of clickers among truthful clickers who clicked yes would be **0.5**

## Part. B
Imagine a medical test for a disease with the following two attributes:  

- The sensitivity is about 0.993. That is, if someone has the disease, there is a probability of 0.993 that they will test positive.
- The specificity is about 0.9999. This means that if someone doesn't have the disease, there is probability of 0.9999 that they will test negative.
- In the general population, incidence of the disease is reasonably rare: about 0.0025% of all people have it (or 0.000025 as a decimal probability).

Suppose someone tests positive. What is the probability that they have the disease?

> Lets start with the mathematical representations of all the probabilities we know, <br>
>   $P(T_{p}|D) = 0.993 \rightarrow$ Probability of testing positive given a person has the disease <br>
>   $P(T_{n}|D_{c}) = 0.9999 \rightarrow$ Probability of testing negative given a person doesn't have the disease <br>
>   $P(D) = 0.000025 \rightarrow$ Probability of a person having the disease <br>
>   $P(D_{c}) = 1- P(D) = 0.999975$ \rightarrow Probability of a person not having the disease <br>
> 
> We'll find the probability of testing positive, given a person doesn't have the disease, i.e.
>   $P(T_{p}|D_{c}) = 1 - P(T_{n}|D_{c}) = 1 - 0.9999 = 0.0001$
> 
> Probability of having the disease given that someone tested positive <br>
>   $P(D|T_{p}) = \frac{P(T_{p}|D)*P(D)}{P(T_{p})}$ <br>
>   $P(D|T_{p}) = \frac{P(T_{p}|D)*P(D)}{P(T_{p}|D)*P(D) + P(T_{p}|D_{c})*P(D_{c})}$ <br>
>   $P(D|T_{p}) = \frac{0.993*0.000025}{0.993*0.000025 + 0.0001*0.999975}$ = 0.1989<br>
>   $P(D|T_{p}) = 0.1989 \rightarrow 19.89\%$ <br>
>
> Therefore, the probability of having the disease given that someone tested positive is **0.1989 or 19.89%**. It has significantly increased from the marginal probability of having the disease which was **0.000025 or 0.0025%**

# 2. Wrangling the Billboard Top 100

using [billboard.csv](./Data/billboard.csv)
``` {r 2setup, include=FALSE}
library(dplyr)
library(tidyverse)

options(dplyr.summarise.inform = FALSE)
```

```{r 2pre}
billboard = read.csv("./Data/billboard.csv")
billboard = billboard[,c("performer", "song", "year", "week", "week_position")]
# Considering the relevant columns as mentioned in the question
head(billboard)
```

## Part. A

Make a table of the top 10 most popular songs since 1958, as measured by the _total number of weeks that a song spent on the Billboard Top 100._  Note that these data end in week 22 of 2021, so the most popular songs of 2021 will not have up-to-the-minute data; please send our apologies to The Weeknd.    

Your table should have __10 rows__ and __3 columns__: `performer`, `song`, and `count`, where `count` represents the number of weeks that song appeared in the Billboard Top 100.  Make sure the entries are sorted in descending order of the `count` variable, so that the more popular songs appear at the top of the table.  Give your table a short caption describing what is shown in the table.  

(_Note_: you'll want to use both `performer` and `song` in any `group_by` operations, to account for the fact that multiple unique songs can share the same title.)  

``` {r 2a, warnings=FALSE}
top_songs_by_week = billboard %>% group_by(song, performer) %>% summarize(count = n())
top10_songs_by_week = head(top_songs_by_week[order(-top_songs_by_week$count),], 10)

knitr::kable(top10_songs_by_week,
             caption = "The Top 10 Songs (Jul 1958 - Jun 2021) by the number of weeks on the Billboard")
```

## Part. B

Is the "musical diversity" of the Billboard Top 100 changing over time?  Let's find out.  We'll measure the musical diversity of given year as _the number of unique songs that appeared in the Billboard Top 100 that year._  Make a line graph that plots this measure of musical diversity over the years.  The x axis should show the year, while the y axis should show the number of unique songs appearing at any position on the Billboard Top 100 chart in any week that year.  For this part, please filter the data set so that it excludes the years 1958 and 2021, since we do not have complete data on either of those years.   Give the figure an informative caption in which you explain what is shown in the figure and comment on any interesting trends you see.

There are number of ways to accomplish the data wrangling here.  We offer you two hints on two possibilities:  

1) You could use two distinct sets of data-wrangling steps.  The first set of steps would get you a table that counts the number of times that a given song appears on the Top 100 in a given year.  The second set of steps operate on the result of the first set of steps; it would count the number of unique songs that appeared on the Top 100 in each year, _irrespective of how many times_ it had appeared.
2) You could use a single set of data-wrangling steps that combines the `length` and `unique` commands. 

``` {r 2b, warnings=FALSE}
musical_diversity = billboard[,c("performer", "song", "year")] 
musical_diversity = musical_diversity %>% filter(!(year %in% c(2021, 1958)))
musical_diversity = unique(musical_diversity)
musical_diversity = musical_diversity %>% group_by(year) %>% summarize(unique=n())
musical_diversity = musical_diversity[order(musical_diversity$year),]

# plot(musical_diversity$year, musical_diversity$unique, type="l", col="red", xlab="Year", ylab="# of unique songs on the Billboard Top-100", main="Musical Diversity")

ggplot(data=musical_diversity, aes(x=year, y=unique, group=1)) +
  geom_line(color="red", size=1) +
  ggtitle("Musical Diversity") +
  xlab("Year") +
  ylab("# of unique songs on the Billboard Top-100")
```

## Part. C

Let's define a "ten-week hit" as a single song that appeared on the Billboard Top 100 for at least ten weeks.  There are 19 artists in U.S. musical history since 1958 who have had _at least 30 songs_ that were "ten-week hits."  Make a bar plot for these 19 artists, showing how many ten-week hits each one had in their musical career.   Give the plot an informative caption in which you explain what is shown.

_Notes_:  

1) You might find this easier to accomplish in two distinct sets of data wrangling steps.
2) Make sure that the individuals names of the artists are readable in your plot, and that they're not all jumbled together.  If you find that your plot isn't readable with vertical bars, you can add a `coord_flip()` layer to your plot to make the bars (and labels) run horizontally instead.
3) By default a bar plot will order the artists in alphabetical order.  This is acceptable to turn in.  But if you'd like to order them according to some other variable, you can use the `fct_reorder` function, described in [this blog post](https://datavizpyr.com/re-ordering-bars-in-barplot-in-r/).  This is optional.

``` {r 2c}
ten_week_hitters = top_songs_by_week %>%
  filter(count >= 10) %>%
  group_by(performer) %>%
  summarize(count = n()) %>%
  filter(count >= 30)

ggplot(data=ten_week_hitters, aes(x=reorder(performer, +count), y=count)) +
  geom_bar(stat="identity", fill="steelblue") +
  ylab("# of 10-week-hits") + xlab("Artist") +
  coord_flip()
```

# 3. Visual story telling part 1: Green Buildings

using [greenbuildings.csv](./Data/greenbuildings.csv) <br>
*\*use raw preview to view the R-code*

``` {r 3, include=FALSE}
library(dplyr)
library(tidyverse)

green_db = read.csv("./Data/greenbuildings.csv")

green_buildings = green_db[green_db$green_rating == 1,]
non_green_buildings = green_db[green_db$green_rating == 0,]
```

``` {r 3a, echo=FALSE}
cat(dim(green_db[green_db$leasing_rate < 0.1, ])[1], "outliers were removed from the data")
```
160 is a significant number of outliers to be discarded with a justification of data cleaning. Let us also see how the outliers modified the median value of the rent
``` {r 3b, echo=FALSE}
cat(
  "Median rent of all green buildings:", median(green_db[green_db$green_rating == 1, "Rent"]),
  "\nMedian rent of all green buildings after removing the outliers:", median(green_db[green_db$green_rating == 1 & green_db$leasing_rate > 0.1, "Rent"])
  )
```
The outlier removal does not seem to have affected the median rate at all, rendering the process completely redundant. Additionally, removing the outliers might have actually cost us a lot in terms of information loss. Let's see how
``` {r 3c, echo=FALSE}
obs_per_cluster = green_db %>% group_by(cluster) %>% summarise(count=n())
removed_per_cluster = green_db[green_db$leasing_rate < 0.1, ] %>% group_by(cluster) %>% summarise(count=n())

merged = merge(obs_per_cluster, removed_per_cluster, by="cluster", suffixes = c(".total",".removed"))
merged$perc_removed = 100*merged$count.removed/merged$count.total
merged = merged[order(-merged$perc_removed),]

plot_merge = merged[merged$perc_removed >= 20, c("cluster", "perc_removed")]
rownames(plot_merge) = NULL
knitr::kable(plot_merge,
             align = "ll",
             col.names = c("cluster", "perc_removed"),
             caption = "The Percentage of cluster rows lost because of the outlier removal (top 27)")

cat("In these 27 clusters, at least 20% of their properties (rows) were removed when the outliers were discarded. From intuition, we may suppose that each cluster might have characteristic rent of its own. So discarding a significant proportion of each cluster is removing vital information. We can also see that the outlier removal removed ",
      dim(green_db[green_db$leasing_rate < 0.1 & green_db$green_rating == 1, ])[1],
     " of the green rated property.")

```

Let's see how the data is cluster-wise. Let us group the clusters and look at the increase in rent for that cluster for a green building, and then summarize it.

``` {r 3d, echo=FALSE, warning=FALSE}
non_green_medians = non_green_buildings %>% group_by(cluster) %>% summarize(median_rent = median(Rent))
merged = merge(green_db, non_green_medians, by="cluster", suffixes=c(".all", ".non_green"))
comparison = merged[merged$green_rating == 1, c("cluster", "Rent","median_rent")]
comparison$difference = comparison$Rent - comparison$median_rent

cat("Median increase in rent for a green property (among all cluster-wise increases): $", median(comparison$difference), sep=)

ggplot(comparison, aes(x=difference)) + 
  geom_histogram(binwidth=3, color="darkblue", fill="lightblue") +
  geom_vline(xintercept = median(comparison$difference), linetype="dashed",color = "red", size=1) +
  geom_text(aes(x=-3+median(comparison$difference), label="Median Rent Increase", y=35), colour="red", angle=90, text=element_text(size=11)) +
  xlab("Rent difference for green buildings") + ylab("Number of clusers")
most_sign_differences = data.frame(table(cut(comparison$difference, 30)))
most_sign_differences = most_sign_differences[order(-most_sign_differences$Freq),]
rownames(most_sign_differences) = NULL
knitr::kable(
  head(most_sign_differences, 3))
```

From the histogram, we can see that the top 3 most frequent buckets (range of rent increases) are as shown in the table above. Among the top-3, 2 are negative, implying the rents for green buildings are lower than the non-green buildings in some of the clusters. That's all aggregated data. Now let's see how the spread of the rent difference is among clusters.

``` {r 3e, echo=FALSE, warning=FALSE}
iqr = quantile(comparison$difference,probs=c(0.25,0.75))
knitr::kable(iqr)
```

the mid-50% of the data is in the range of (-0.4125,6.3475) meaning we can expect our rent to increase by this amount (per sqaure feet) if we ignore all other factors that might effect the rent. What other factors might be having an effect on the rent?

``` {r 3f, echo=FALSE, warning=FALSE, fig.width=8, fig.height=8}
par(mfrow=c(2,2))
plot(green_db$leasing_rate, green_db$Rent, ylab="Rent", xlab="Leasing Rate")
plot(green_db$Electricity_Costs, green_db$Rent, ylab="Rent", xlab="Electricity cost")
plot(green_db$cd_total_07, green_db$Rent, ylab="Rent", xlab="Cooling  Demand")
plot(green_db$hd_total07, green_db$Rent, ylab="Rent", xlab="Heating  Demand")
```

We can see, although not clearly, that the rent varies with leasing rate, electricity cost, cooling demand, heating demand, etc. all of which depend on the location of the property. This information about our propery under consideration being on East Cesar Chavez, just across I-35 from downtown is never being considered in the analysis. Considering these variations might also give us a more *reliable* prediction of the rent difference.

One more factor considered in the analysis was 100% or 90% occupancy from day-1 of the property being ready for occupancy. This is rarely the case. Let us assume the median rent difference of $3 per square fit per year. and let us consider the property being occupied 100% over a span of some years

``` {r 3g, echo=FALSE}
y3 = c(.33, .66, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)
y5 = c(.2, .4, .6, .8, 1.0, 1.0, 1.0, 1.0)
y8 = c(.125, .25, .375, .5, .625, .75, .875, 1.0)
annual_income_full_occupancy = 250000*3

year3 = c()
year5 = c()
year8 = c()

for(i in 1:8){
  year3 = c(year3, annual_income_full_occupancy*y3[i])
  year5 = c(year5, annual_income_full_occupancy*y5[i])
  year8 = c(year8, annual_income_full_occupancy*y8[i])
}

target_occupancy_100 = c(
  round(3 + ((5000000-sum(year3[1:3]))/annual_income_full_occupancy), 2),
  round(5 + ((5000000-sum(year5[1:5]))/annual_income_full_occupancy), 2),
  round(8 + ((5000000-sum(year8[1:8]))/annual_income_full_occupancy), 2)
)

y3 = y3*0.75
y5 = y3*0.75
y8 = y3*0.75

year3 = c()
year5 = c()
year8 = c()

for(i in 1:8){
  year3 = c(year3, annual_income_full_occupancy*y3[i])
  year5 = c(year5, annual_income_full_occupancy*y5[i])
  year8 = c(year8, annual_income_full_occupancy*y8[i])
}

target_occupancy_75 = c(
  round(3 + ((5000000-sum(year3[1:3]))/(annual_income_full_occupancy*0.75)), 2),
  round(5 + ((5000000-sum(year5[1:5]))/(annual_income_full_occupancy*0.75)), 2),
  round(8 + ((5000000-sum(year8[1:8]))/(annual_income_full_occupancy*0.75)), 2)
)

df = data.frame(target_occupancy_100, target_occupancy_75)
rownames(df) = c("Target occupancy reached by year 3", "Target occupancy reached by year 5", "Target occupancy reached by year 8")
knitr::kable(df,
             align = "ll",
             caption = "The Number of years required to recuperate the premium for green certification")

```

With this analysis, 7.7 years is still the best case scenario of recuperating the premium spent on a green certification. For a more realistic scenario, we can see that the cost will be recuperated before making any profits after at least 10 years of the construction. This gives a more realistic picture by just considering the rent increase of the median cluster disregarding all other factors. We can still suggest the developer to go for a linear regression model to get a more accurate prediction of the returns. Additionally, the linear model would also give a sense the role a particular factor plays in the estimation of the returns.

# 4. Visual story telling part 2: Capital Metro data

using [capmetro_UT.csv](./Data/capmetro_UT.csv) <br>
*\*use raw preview to view the R-code*



# 5. Portfolio Modeling

For this question, we decided to do 3 different tests with different types of ETFs to understand the different gains and losses through bootstrapping. 

We first used Oil and Gas commodity ETFs to understand how much we would potentially be gaining or losing if we were to invest in them. 

```{r 5a, include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)

#### Now use a bootstrap approach
#### With more stocks

#Commodities

#USO: https://etfdb.com/etf/USO/
#UNG: https://etfdb.com/etf/UNG/
#OILK: https://etfdb.com/etf/OILK/
#GAZ: https://etfdb.com/etf/GAZ/
#RJN: https://etfdb.com/etf/RJN/

mystocks = c("USO", "UNG", "OILK", "GAZ", "RJN")
myprices = getSymbols(mystocks, from = "2017-08-13")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind( ClCl(USOa),
                     ClCl(UNGa),
                     ClCl(OILKa),
                     ClCl(GAZa),
                     ClCl(RJNa))
```

These are the stocks we used for this portfolio: 

USO: United States Oil Fund LP

UNG: United States Natural Gas Fund LP

OILK: ProShares K-1 Free Crude Oil Strategy ETF

GAZ: iPath Series B Bloomberg Natural Gas Subindex Total Return ETN

RJN: Elements Rogers International Commodity Index-Energy Total Return ETN

```{r 5b, echo = FALSE}
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```

From the coefficients we can see the difference in each stocks and how they differ from day to day. We started measuring from 5 years back till today. 

We then made a pairs plot to see each ETF against each other

```{r 5c, echo = FALSE}
# Compute the returns from the closing prices
pairs(all_returns)
```

We then simulate a random day to where we distribute each ETF to our $100K capital equally and then find the sum to compute our new total wealth at the end of the random day. 

```{r 5d, echo = FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.2,0.2,0.2, 0.2, 0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
holdings
total_wealth = sum(holdings)
total_wealth
```

So as we can see on a random day we went from $100,000 in capital to the output.

We then observed the change over a time period of two weeks. 

```{r 5e, echo = FALSE}
# Now loop over two trading weeks
# let's run the following block of code 5 or 6 times
# to eyeball the variability in performance trajectories

## begin block
total_wealth = 100000
weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')
## end block
```

So we can see the change in our capital over the time of 20 days.

So now we bootstrap this model.

```{r 5f, echo = FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

So we can see the distribution of capital after bootstrapping. It is mostly normal.

```{r 5g, echo = FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

# note: this is  a negative number (a loss, e.g. -500), but we conventionally
# express VaR as a positive number (e.g. 500)
```

So this is our ultimate gain/loss, which can vary from time to time. 

Our next portfolio is emerging market equities, which can be important for people who are interested in investing in new markets which can be relatively risky.

```{r 5h, include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)

#### Now use a bootstrap approach
#### With more stocks

#Commodities

#VTO: https://etfdb.com/etf/VTO/
#IEMG: https://etfdb.com/etf/IEMG/
#GEM: https://etfdb.com/etf/GEM/
#DEM: https://etfdb.com/etf/DEM/
#PXH: https://etfdb.com/etf/PXH/

mystocks = c("VTO", "IEMG", "GEM", "DEM", "PXH")
myprices = getSymbols(mystocks, from = "2017-08-13")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind( ClCl(VTOa),
                     ClCl(IEMGa),
                     ClCl(GEMa),
                     ClCl(DEMa),
                     ClCl(PXHa))
```

These are the stocks we used for this portfolio: 

VTO: Vanguard FTSE Emerging Markets ETF

IEMG: iShares Core MSCI Emerging Markets ETF

GEM: Goldman Sachs ActiveBeta Emerging Markets Equity ETF

DEM: WisdomTree Emerging Markets High Dividend Fund

PXH: Invesco FTSE RAFI Emerging Markets ETF

```{r 5i, echo = FALSE}
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```

From the coefficients we can see the difference in each stocks and how they differ from day to day. We started measuring from 5 years back till today. 

We then made a pairs plot to see each ETF against each other

```{r 5j, echo = FALSE}
# Compute the returns from the closing prices
pairs(all_returns)
```

We then simulate a random day to where we distribute each ETF to our $100K capital equally and then find the sum to compute our new total wealth at the end of the random day. 

```{r 5k, echo = FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.2,0.2,0.2, 0.2, 0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
holdings
total_wealth = sum(holdings)
total_wealth
```

So as we can see on a random day we went from $100,000 in capital to output [1]. This amount varies from day to day.

We then observed the change over a time period of two weeks. 

```{r 5l, echo = FALSE}
# Now loop over two trading weeks
# let's run the following block of code 5 or 6 times
# to eyeball the variability in performance trajectories

## begin block
total_wealth = 100000
weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')
## end block
```

So we can see the change in our capital over the time of 20 days.

So now we bootstrap this model.

```{r 5m, echo = FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

So we can see the distribution of capital after bootstrapping. It is mostly normal. 

```{r 5n, echo = FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

# note: this is  a negative number (a loss, e.g. -500), but we conventionally
# express VaR as a positive number (e.g. 500)
```

So our ultimate gain/loss would be output [1]. This could vary due to randomization in bootstrapping. 

The last portfolio we used is with Health and Biotech Equities, which is slightly less common but still has a large capital. 

```{r 5o, include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)

#### Now use a bootstrap approach
#### With more stocks

#Commodities

#XHE: https://etfdb.com/etf/XHE/
#PSCH: https://etfdb.com/etf/PSCH/
#IHE: https://etfdb.com/etf/IHE/
#PBE: https://etfdb.com/etf/PBE/
#FHLC: https://etfdb.com/etf/FHLC/

mystocks = c("XHE", "PSCH", "IHE", "PBE", "FHLC")
myprices = getSymbols(mystocks, from = "2017-08-13")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind( ClCl(XHEa),
                     ClCl(PSCHa),
                     ClCl(IHEa),
                     ClCl(PBEa),
                     ClCl(FHLCa))
```

These are the stocks we used for this portfolio: 

XHE: SPDR S&P Health Care Equipment ETF

PSCH: Invesco S&P SmallCap Health Care ETF

IHE: IShares U.S. Pharmaceuticals ETF

PBE: Invesco Dynamic Biotechnology & Genome ETF

FHLC: Fidelity MSCI Health Care Index ETF

```{r 5p, echo = FALSE}
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```

From the coefficients we can see the difference in each stocks and how they differ from day to day. We started measuring from 5 years back till today. 

We then made a pairs plot to see each ETF against each other

```{r 5q, echo = FALSE}
# Compute the returns from the closing prices
pairs(all_returns)
```

We then simulate a random day to where we distribute each ETF to our $100K capital equally and then find the sum to compute our new total wealth at the end of the random day. 

```{r 5r, echo = FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.2,0.2,0.2, 0.2, 0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
holdings
total_wealth = sum(holdings)
total_wealth
```

So as we can see on a random day we went from $100,000 in capital to output[1]. 

We then observed the change over a time period of two weeks. 

```{r 5s, echo = FALSE}
# Now loop over two trading weeks
# let's run the following block of code 5 or 6 times
# to eyeball the variability in performance trajectories

## begin block
total_wealth = 100000
weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')
## end block
```

So we can see the change in our capital over the time of 20 days.

So now we bootstrap this model.

```{r 5t, echo = FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

So we can see the distribution of capital after bootstrapping. It is mostly normal. 

```{r 5u, echo = FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

# note: this is  a negative number (a loss, e.g. -500), but we conventionally
# express VaR as a positive number (e.g. 500)
```

So our ultimate gain/loss would be output [1], however this could change due to randomization in bootstrapping.

When we compare all three, we can see which portfolio is better to invest in, as it gains more capital or loses the least. 


# 6. Clustering and PCA
```{r 6a, include=FALSE}
library(tidyverse)
library(cluster)
library(factoextra)
wine = read.csv("./Data/wine.csv")
wine=wine[ , names(wine) != "color"]
wine$new <- c(wine$color)
```

```{r PCA_Analysis}
pr.out = prcomp(wine, ncp=5, scale = TRUE)
names(pr.out)
pr.out$center #means of variables
pr.out$scale #standard deviations of variables
pr.out$rotation
dim(pr.out$x)
biplot(pr.out, scale =0)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
pr.out$sdev
pr.var = pr.out$sdev^2
pr.var
pve = pr.var/sum(pr.var)
pve
#PVE explained by each component
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = 'b')
#the cumulative PVE
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0,1), type = 'b')
```

PCA allowed us to reduce the dimensionality of this very large dataset. This gave us a deeper insight on the key variables in the dataset. This unsupervised learning tool is valuable, as the information generated from this model can be applied in supervised learning methods. From this model, 12 different principal components were generated. The graph shows that PC1 and PC2 explain a greater proportion of variance in the dataset than the other principal components do. This is the general trend, so it was expected these principal components would capture the most variance. In PC1, the variables with the greatest magnitude were total sulfur dioxide, free sulfur dioxide, and volatile acidity. For PC2 alcohol had the greatest magnitude. This gives insight on where the highest variance exists in the dataset, as the first PC attempts to account for the highest variance and then the second is the next highest, and so forth.

```{r K_Means_Clustering}
wine = read.csv("./Data/wine.csv")
wine=wine[ , names(wine) != "color"]

wine <- scale(wine)
head(wine)
distance <- get_dist(wine)
set.seed(2)
km <- kmeans(wine, centers = 2)
km$cluster
#plot(wine, col = (km$cluster + 1), main = "K-Means Clustering Results with K =2", xlab = "", ylab = "", pch = 20, cex =2)

# get cluster means 
aggregate(wine ,by=list(km$cluster), FUN=mean)

# append cluster assignment
mydata <- data.frame(wine, km$cluster)

fit <- kmeans(mydata, 2)
library(cluster)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE,
   labels=2, lines=0)
```

For this dataset, K Means Clustering was chosen over PCA to analyze different wines and their 11 associated chemical properties. K Means Clustering made more sense for this probloem as it will identify groups not explicity labeled in the data, which is exactly what we were testing. The goal of this unsupervised learning model was to see if these wines can be clustered based on similar properties. The 'color' column was removed, because we were trying to create a model that would correctly classify the wines as red or white. 
#The K Means Clusering model did correctly cluster some of the red and white wines, but it was not 100% accurate. To test how well our K Means Clustering model was at classifying red vs white wines, we added back the color column and studied if all the red wines and white wines were assigned the same cluster. It appeared that the red wines were correctly classified, but several of the white wines were not. Overall, this model was able to distinguish between red and white wine but did not have a perfect accuracy. 
#Finally, we tested if the K Means Clustering model was able to correctly distinguish between high and low quality wines. The study this, the clusters column and the quality columns were compared and there did not appear to be any clustering based on quality. For example, a wine rated as a 3 was clustered in the same cluster as wines getting a rating of 6+. Further tests could be run to study why this was occuring.

# 7. Market Segmentation

The social marketing data set consists of users and their different tweets categorized into 35 main topics. Our goal was to define a "market segment" using this data set. We initially took a look at the data and decided which type of analysis was to fit best to define the market segment. 

Since there were likely to be groups of different types of tweets, we decided to take a look at hierarchical clustering, however this did not end with favorable results, as there are too many splits on the decision tree and whichever k we chose was leading to an uneven distribution. Even with a K++ cluster we ended up with very high numbers, and decided to move to the next type of analysis. 

```{r 7a, include = FALSE}
library(tidyverse)
library(ggplot2)
library(ggcorrplot)

social_marketing <- read.csv("./Data/social_marketing.csv", row.names = 1)

social_marketing <- cbind(User = rownames(social_marketing), social_marketing)
rownames(social_marketing) <- 1:nrow(social_marketing)

social_results = social_marketing %>%
  group_by(User) %>% 
  select(-User) %>%
  summarize_all(mean) %>%
  column_to_rownames(var="User")
```

We then attempted PCA to understand through which factors leads in the most variance of the data set. This split could potentially be defined as the market segment, so we decided to take a look into that. 

We ran a quick few plots to see the relationship between the User and various categories. In the following graphs we can see the relationship between chatter tweets by the user and travel tweets by the user respectively.

```{r 7b, echo = FALSE}
ggplot(rownames_to_column(social_results, "User")) + 
  geom_col(aes(x=reorder(User, -chatter), y = chatter)) + 
  coord_flip()

ggplot(rownames_to_column(social_results, "User")) + 
  geom_col(aes(x=reorder(User, -travel), y = travel)) + 
  coord_flip()
```

We can see that there is some sort of relationship, so we proceeded with PCA.

We took a more general look at the correlation between each variable in the following graph.

```{r 7c, echo = FALSE}
# a look at the correlation matrix
#cor(social_results)

# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(social_results), hc.order = TRUE)
```

Looking through the correlation plot, we can see some variables that have a fairly strong correlation with each other.

Next we took a look at the distribution of variance among the data set. This will then split our PCs so we can observe them individually. 

```{r 7d, echo = FALSE}
# Now look at PCA of the (average) survey responses.  
PCApilot = prcomp(social_results, scale=TRUE)

## variance plot
plot(PCApilot)
summary(PCApilot)
```

We can see that around PC16 gives us around 75% of the cumulative proportion, which is around 1/2 of the PCs. So we decided to take a look at a few.

The following table shows the first 10 PCs. We decided to take a look at the first three to see if we can draw a conclusion.

```{r 7e, echo = FALSE}
# first few pcs
# try interpreting the loadings
# the question to ask is: "which variables does this load heavily on (positive and negatively)?"
round(PCApilot$rotation[,1:10],2) 

# create a tidy summary of the loadings
loadings_summary = PCApilot$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Question')
```

```{r 7f, echo = FALSE}
# not sure yet
loadings_summary %>%
  select(Question, PC1) %>%
  arrange(desc(PC1))
```

After looking at PC1, we did not find any obvious meaning of the vectors, meaning that it is likely noise, so we proceeded to the next PC.

```{r 7g, echo = FALSE}
#  this looks like influencer tweets vs serious tweets
loadings_summary %>%
  select(Question, PC2) %>%
  arrange(desc(PC2))
```

Looking at the categories for the tweets and how they are arranged, we can see that there is a separation between recreational tweets such as cooking, shopping, fashion, and serious tweets such as politics and news.

Now lets look at PC3.

```{r 7h, echo = FALSE}
loadings_summary %>%
  select(Question, PC3) %>%
  arrange(desc(PC3))
```

PC3 looks similar to PC2, except that some of the variables have different vectors. We can conclude that this is also similar to recreational or influencer tweets vs serious tweets.

We then made some plots using the PCs that worked. 

```{r 7i, include = FALSE}
# Let's make some plots of the shows themselves in 
# PC space, i.e. the space of summary variables we've created

social_marketing <- read.csv("./data/social_marketing.csv", row.names = 1)

social_marketing = merge(social_marketing, PCApilot$x[,1:3], by="row.names")
social_marketing = rename(social_marketing, social_marketing = Row.names)
```

```{r 7j, echo = FALSE}

# let's plot in PC1 space
# We might feel good calling PC1 the "quality drama" PC
ggplot(social_marketing) + 
  geom_col(aes(x=reorder(social_marketing, PC2), y=PC2)) + 
  coord_flip()

```

```{r 7k, echo = FALSE}
# looks like a "lighthearted vs serious" PC
ggplot(social_marketing) + 
  geom_col(aes(x=reorder(social_marketing, PC3), y=PC3)) + 
  coord_flip()
```

Both of these graphs look similar but have a distinct distribution

```{r 7l, echo = FALSE}
# principal component regression: predicted engagement
lm1 = lm(travel ~ PC1 + PC2 + PC3, data=social_marketing)
summary(lm1)

# gross ratings points
lm2 = lm(college_uni ~ PC1 + PC2 + PC3, data=social_marketing)
summary(lm2)

# Conclusion: we can predict engagement and ratings
# with PCA summaries of the pilot survey.
# probably too much variance to regress on all survey questions!
# since the sample size isn't too large here.
plot(travel ~ fitted(lm1), data=social_marketing)
plot(college_uni ~ fitted(lm2), data=social_marketing)
```

After plotting linear models with all 3 PCs, we can see that the main difference in tweets is recreational vs serious. We can use this to define our market segment. In this dataset, the market is separated by the types of tweets that are recreational and serious. It shows the main differences in tweets by users. 

# 8. The Reuters corpus

## Objective: To build two separate models for predicting the author of an article on the basis of that article's textual content.The first part of this exercise derives a lot of its content from NaiveBayes.R which was discussed in class. To start with let's create the reader file

```{r 8a, include=FALSE}
library(tm)
library(caret)
library(e1071)
library(randomForest)
```

read test file

```{r 8b}
readerPlain = function(fname){readPlain(elem=list(content=readLines(fname)),
                                        id=fname, language='en') }
author_dirs_train = Sys.glob('./Data/ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL
for(author in author_dirs_train) {
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  author_name = substring(author, first=21)
  labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}
```

read test data

```{r 8c}
author_dirs_test = Sys.glob('./Data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  author_name = substring(author, first=20)
  labels_test = append(labels_test, rep(author_name, length(files_to_add))) }
```

To deal with the words in the test set that we never saw in the training set, the training and test sets were combined. This will be used later to create a single document term matrix with all words from the training and test datasets.

```{r 8d}
file_lists = append(file_list_train,file_list_test)
labels = NULL
labels <- unique(append(labels_train, labels_test))
all_docs = lapply(file_lists, readerPlain) 
names(all_docs) = file_lists
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus = Corpus(VectorSource(all_docs))
#names(my_corpus) = names(all_docs)
```

```{r 8e}
# Preprocessing
my_corpus= tm_map(my_corpus, content_transformer(tolower)) # make everything
my_corpus= tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus= tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
DTM

```

We can see that the sparsity of the document is very high. Let us now remove all elements which have a sparse factor greater than 0.975.

```{r 8f}
#inspect(DTM[1:10,1:5])
DTM = removeSparseTerms(DTM, 0.975)
DTM
```

The first model was built using Naive Bayes, which was discussed in class.

Naive Bayes: Let us first convert the DTM to a data matrix then we separate the training set and test set by just taking the first 2500 and the next 2500 entries in the DTM. This should work well simply because our file lists were correctly ordered based on train and test.We can see that here.

```{r 8g}
X = as.matrix(DTM)

file_lists[2490:2510]

# split the data into train and test
X_train = X[1:2500,]
labels = unique(labels)
```

Now let us calculate the term level weights for each author by applying the Laplace smoothing factor.

```{r 8h}
smooth_count = 1/nrow(X_train)
for(i in 1:50)
{
  w_name <- paste("w",labels[i], sep = "_")
  temp <- colSums(X_train[(50*(i-1)+1):(50*i),] + smooth_count)
  assign(w_name, temp/sum(temp))
}
```

Now using the above weight vectors, let us predict the author name of the test data. We do this by calculation the log probabilities of all documents across all authors. The author with the highest value will be the most probable author for that document.

```{r 8i}
X_test =  X[2501:5000,]

pred = matrix(, nrow = 2500, ncol = 51) 
for(i in 1:2500)
{
  for(j in 1:50)
    {
    w_name <- paste("w",labels[j], sep = "_")
    pred[i,j] = sum(X_test[i,]*log(get(w_name))) }
}

pred[1:10,1:5]
```

Let us now create a list with the predicted authors for each document by finding the highest probable authors for each document

```{r 8j}

for (i in 1:2500)
{
  pred[i,51] = which.max(pred[i,])
}

predicted = data.frame(
  actual_author = as.factor(rep(1:50, each=50)),
  pred_author = as.factor(pred[,51])
)

confusionMatrix(predicted$pred_author,predicted$actual_author)
```

The accuracy of the Naive Bayes classification model is 60.24%. Since some authors had works that were very similar, it was tough to distinguish them. These authors have a similar "bag of words" making it safe to assume that they write on similar topics. In fact, we can see several of these occurances in the confusion matrix. For example, look at author 14 and 19. 23 articles of author JanLopatka (author number 14) have been classified as written by JohnMastrini (author number 19). 11 of John's articles have again been classified as written by Jan. This means both authors are writing about similar topics.

Lets go ahead and verify that! Below I used random forest to verfy this.

```{r 8k}
actual_author = rep(rep(1:50,each=50),2)
author = as.data.frame(X)
colnames(author) = make.names(colnames(author))
author$actual_author=actual_author 
author$actual_author=as.factor(author$actual_author)

# SPLIT DATA
# The data was split into the training and test datasets.
# A seed was also set so this code can be replicated. 

author_train=author[1:2500,]
author_test=author[2501:5000,]
set.seed(23432)
rf_author=randomForest(actual_author~.,data=author_train) 

predicted_author=predict(rf_author,newdata=author_test) 
confusionMatrix(predicted_author,author_test$actual_author)
```

Using the training and test corpus of Reuters dataset, we built 2 classifiers - Naive Bayes and Random Forest. Out of these two, Random Forest gives slightly better prediction than Naive Bayes. Even though the accuracy is slightly higher for Random Forest, we would still choose Naive Bayes because it is much less complex than Random Forest. It is also less computationally intensive compared to Random Forest.

We wanted improve our model so we tried to fit PCA and ran the model again

```{r 8l, include=FALSE}
library(tm)
library(magrittr) 
library(slam)
library(proxy)
#install.packages("glmnet")
library(glmnet)
library(caret) 
library(dplyr)
library(naivebayes)
#install.packages('naivebayes')
library(randomForest)
library(e1071) 
library(caret)
```

```{r 8m}
text_data_preprocess = function(pp) { 
  writer_list = list.files(pp)
  read_f_list = c()
  
  readerPlain = function(fname) { 
    readPlain(elem=list(content=readLines(fname)),id=fname, language='en') }
  
  for ( i in writer_list){ 
    read_f_list = c(read_f_list,Sys.glob(paste0(pp,i,'/*.txt')))
  }  
  
  all_Doc = lapply(read_f_list, readerPlain)
  mynames = read_f_list %>%
    { strsplit(., '/', fixed=TRUE) } %>%
    { lapply(., tail, n=2) } %>%
    { lapply(., paste0, collapse = '') } %>% unlist  
  names(all_Doc) = mynames
  
  documents_raw = Corpus(VectorSource(all_Doc))
  
  my_documents = documents_raw
  my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
  my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
  my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
  my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
  my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
  return(my_documents)
}
```

```{r 8n}
get_y = function(pp)
{ writer_list = list.files(pp) 
y = c()
for ( i in writer_list){
  y = c(y, rep(i, times = length(list.files(paste0(pp,i)))))
}
return(y) 
}

```

we build our "document term matrix" and prepare for PCA by removing sparse terms, sort columns by alphabetical order, and remove the zero-sum columns.

```{r 8o}
my_documents = text_data_preprocess('./Data/ReutersC50/C50train/')
y = get_y('./Data/ReutersC50/C50train/') 
DTM_all = DocumentTermMatrix(my_documents)
DTM_all_d = removeSparseTerms(DTM_all, 0.95)
DTM_all_s = DTM_all_d[ ,order(DTM_all_d$dimnames$Terms)]
tfidf_all = weightTfIdf(DTM_all_d) 
tfidf_matrix = as.matrix(tfidf_all)
scrub_cols = which(colSums(tfidf_matrix) == 0)
pre_pca_1 = tfidf_matrix[,-scrub_cols]
```

we will follow the same process for test also and intersect the common results from train and test

```{r 8p}
test_documents = text_data_preprocess('./Data/ReutersC50/C50test/')
y_test = get_y('./Data//ReutersC50/C50test')
DTM_test = DocumentTermMatrix(test_documents) 
DTM_test_s = DTM_test[, order(DTM_test$dimnames$Terms)]
Term_inter = intersect(Terms(DTM_test_s), colnames(pre_pca_1))
```

running PCA using our training document term matrix, using the vocabulary intersection as columns. we will fit naive bayes

```{r 8q}
mycorpus = text_data_preprocess('./Data/ReutersC50/C50train/')
labels = get_y('./Data/ReutersC50/C50train/')
DTM = DocumentTermMatrix(mycorpus) 
DTM = removeSparseTerms(DTM, 0.975)
tfidf_train = weightTfIdf(DTM)
X = as.matrix(tfidf_train)

mycorpus2 = text_data_preprocess('./Data/ReutersC50/C50test/')
labels2 = get_y('./Data/ReutersC50/C50test/')
DTM2=DocumentTermMatrix(mycorpus2) 
DTM2=removeSparseTerms(DTM2,0.975) 
tfidf_test = weightTfIdf(DTM2)
x2=as.matrix(tfidf_test)

words=colnames(X)
words2=colnames(x2)
W=words[!(words %in% words2)] 
W2=words2[!(words2 %in% words)]
words_matrix=matrix(0,nrow=nrow(x2), ncol=length(W)) 
colnames(words_matrix)=W
words_matrix2=matrix(0,nrow=nrow(X), ncol=length(W2)) 
colnames(words_matrix2)=W2
train_matrix=cbind(X,words_matrix2)
test_matrix=cbind(x2,words_matrix)

# check the test accuracy
set.seed(1)
test_matrix=as.data.frame(test_matrix) 
train_matrix=as.data.frame(train_matrix)
nb = naive_bayes(x=train_matrix,y=as.factor(labels),laplace=1)
predNB=predict(nb,test_matrix)
actual = rep(1:50,each=50)

TestTable = table(predNB,actual)
correct = 0
for (i in seq(1,50)){
  correct = correct + TestTable[i,i] }
NB_accuracy = correct/2500
print(NB_accuracy)
```

```{r 8r}
NB_confusion = confusionMatrix(table(predNB,labels)) 
NB_class= as.data.frame(NB_confusion$byClass)
NB_class[order(-NB_class$Sensitivity),][1]
```
The Naive Bayes model prediction accuracy is somewhat low. A different model may have better predictive accuracy 0.4396. From confusion matrix results we can say that the model predicted well for a few authors like LynnleyBrowning,MatthewBunce and RobinSidel.

To further improve upon our accuracy we tried random forest model
```{r 8s}
set.seed(1)
RF = randomForest(y=as.factor(labels), x=train_matrix,ntrees=500) 
pr = predict(RF, test_matrix, type = "response")
TestTable2 = table(pr, actual)
correct2 = 0
for (i in seq(1,50)){
  correct2 = correct2 + TestTable2[i,i] }
RF_accuracy = correct2/2500 
print(RF_accuracy)
# he random forest model was a good bit better at 0.6212

RF_confusion = confusionMatrix(table(pr,labels))
RF_class= as.data.frame(RF_confusion$byClass) 
RF_class[order(-RF_class$Sensitivity),][1]

```

The model predict well for a few authors like FumikoFujisaki, JimGilchrist and LynnleyBrowning. We can also see that random forest model, on average, have a better accuracy then the other model we did.

# 9. Association rule mining

load all the libraries required fo the analysis

```{r 9a}

library(readr)
library(tidyverse) 
library(arules) 
library(arulesViz)
library(reshape)
```

load the groceries.txt data

```{r 9b}
df_groceries=read.csv("./Data/groceries.txt")
head(df_groceries,9)
```

From the above table we can understand that, rows and columns are not being ordered correctly. Let's check the number of fields in the data and load the data again.

```{r 9c}
max(count.fields('./Data/groceries.txt',sep = ','))
```

```{r 9d}
df_groc = read.csv("./Data/groceries.txt", header = FALSE, col.names =
                  paste0("V",seq_len(32)), fill = TRUE)
head(df_groc,10)
dim(df_groc)
```

This data has 9835 rows. Let's get the transaction and bind it with the dataframe

```{r 9e}
row_number=1:nrow(df_groc)
df_groc=cbind(row_number,df_groc)
head(df_groc)
```

By noticing the dataframe we can understand that data is not at one transaction per row, each variable has different number of levels, we need to unstack then order it by transactions and remove the nulls then add back to the dataframe using split function

```{r 9f}
df_groc1 = melt(df_groc,id=c("row_number"))
df_groc1 = df_groc1[order(df_groc1$row_number),]
df_groc1[df_groc1==""] <- NA
dim(df_groc1)
df_groc1 = na.omit(df_groc1)
# after removing the nulls check the shape of the data
dim(df_groc1)
head(df_groc1)
df_groc1$row_number = factor(df_groc1$row_number)
head(df_groc1)
```

Above code the output of words in the unstacked format. Let's create a list of baskets. In order to do that, split a data into a list of items for each basket then perform lapply() function after removing duplicates. lapply() function helps us in applying functions on list/df objects and returns a list object of the same length.

```{r 9g}
df_groc1 = split(x=df_groc1$value, f=df_groc1$row_number)
head(df_groc1)

df_groc1 = lapply(df_groc1, unique)
head(df_groc1)
```

Data is ready to be inserted into the apriori model.We could have run the read transaction function where in we don't have to do any data wrangling step for that, We can first read in our grocery list by letting each row of the data as a basket of one shopping list then we can seperate each row by comma as items in each basket but we wanted to try manual method to understand the preprocessing steps [ groceries = read.transactions('./Data/groceries.txt',format = 'basket', sep = ',',rm.duplicates = FALSE)] grocery\<- as(groceries, "transactions")

Lets cast df_groc1 variable as a special arules "transactions" class after that run the apriori algorithm with support \> .01 & confidence \>.5 & length (\# items) \<= 4

```{r 9h}
grocery_trans = as(df_groc1, "transactions")
grocery_trans
grocery_rules = apriori(grocery_trans, parameter=list(support=.01, confidence=.5, maxlen=4))
```

```{r 9i}
arules::inspect(grocery_rules)
```

```{r 9j}
library(arulesViz)
plot(grocery_rules, method = "graph", 
     measure = "confidence", shading = "lift")
```

The apriori method is used to detect the relationships between the various goods in the various baskets that were loaded. We originally used a tougher criterion of support value = 0.01 and confidence = 0.5, and the results were primarily 'whole milk' and 'other vegetables'. Based on this finding, we discovered that 'whole milk' and 'other veggies' account for a sizable part of purchases at this grocery shop.

As a result, we propose placing 'whole milk' and 'other vegetables' in the centre of our store,which not only enhances our customers' shopping experience by allowing them to get what they need fast, but also boosts exposure of other items.

Now we should identify more frequent itemsets .In order to do that we have to play with cutoff values choose one pair that makes logical.First changing the support cutoff and keeping confidence cutoff constant

```{r 9k}
grocery_rules= apriori(grocery_trans, parameter=list(support=.005, confidence=.5, maxlen=4))
arules::inspect(grocery_rules)     
```

```{r 9l}
plot(grocery_rules, method = "graph", 
     measure = "confidence", shading = "lift")
```

The above model gave 120 results so not including the output.

```{r 9m}
grocery_rules = apriori(grocery_trans, parameter=list(support=.002, confidence=.8, 
                                                 maxlen=4))
arules::inspect(grocery_rules)

```

Again, nothing new. All that we would expect. I am decreasing the support even further and increasing the maxlen to 5

```{r 9n}
grocery_rules <- apriori(grocery_trans, parameter=list(support=.0015, 
                                                 confidence=.9, maxlen=5))
arules::inspect(grocery_rules)

```

```{r 9o}
plot(grocery_rules, method = "graph", 
     measure = "confidence", shading = "lift")
```

This time also we were expecting whole milk and other combinations but surprisingly we got the item liquour, rd/blush wine and bottled beer with 19 occurences.This has support of 0.0015 so this seems to be optimal support, confidence and the maxlen.

1. Whole milk goes with cream cheese/butter/yogurt/sugar and root/other vegetables/tropical fruits/domestic eggs. These all look like a regular grocery shopping and we can say all the other items have very good associations as well.

2. Customers that come in to buy alcohol usually buy many different types of alcohol.

So the association of liquor and red/blush wine with bottled beer makes perfect sense, and its confidence conveys the same narrative. Placing these above itemsets next to each other in the grocery store will make shopping very convenient for the customers.
