# 1. Probability practice

## Part. A
Visitors to your website are asked to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories: Random Clicker (RC), and Truthful Clicker (TC). There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the information that the expected fraction of random clickers is 0.3.  After a trial period, you get the following survey results: 65\% said Yes and 35\% said No.   What fraction of people who are truthful clickers answered yes?  Hint: use the rule of total probability.

> Lets start with the mathematical representations of all the fractions we know, <br>
> $P(Y) = 0.65 \rightarrow$ Marginal probability of a clicker saying Yes <br>
> $P(N) = 0.35 \rightarrow$ Marginal probability of a clicker saying No <br>
> $P(R_{c}) = 0.3 \rightarrow$ Probability of a clicker being a random clicker <br>
> $P(T_{c}) = 0.7 \rightarrow$ Probability of a clicker being a truthful clicker <br>
> 
> To find the fraction of people truthful clickers who answered yes, we find the value of $P(Y|T_{c})$
> 
> We know that, <br>
>   $P(Y) = P(Y|R_{c}) + P(Y|T_{c})$
> 
> Rearranging, <br>
>   $P(Y|T_{c}) = P(Y) - P(Y|R_{c})$
> 
> If a random clicker clicking *Yes* or *No* is equally likely, then the probability (or fraction) of Random clickers, clicking yes > is (Fraction of users among the 0.3 random clickers, clicking *Yes*), <br>
>   $P(Y|R_{c}) = 0.5*0.3 = 0.15$
> 
> Substituting in the rearranged equation, <br>
>   $P(Y|T_{c}) = 0.65 - 0.15$ <br>
>   $P(Y|T_{c}) = 0.5$
>
> Therefore, the fraction of clickers among truthful clickers who clicked yes would be **0.5**

## Part. B
Imagine a medical test for a disease with the following two attributes:  

- The sensitivity is about 0.993. That is, if someone has the disease, there is a probability of 0.993 that they will test positive.
- The specificity is about 0.9999. This means that if someone doesn't have the disease, there is probability of 0.9999 that they will test negative.
- In the general population, incidence of the disease is reasonably rare: about 0.0025% of all people have it (or 0.000025 as a decimal probability).

Suppose someone tests positive. What is the probability that they have the disease?

> Lets start with the mathematical representations of all the probabilities we know, <br>
>   $P(T_{p}|D) = 0.993 \rightarrow$ Probability of testing positive given a person has the disease <br>
>   $P(T_{n}|D_{c}) = 0.9999 \rightarrow$ Probability of testing negative given a person doesn't have the disease <br>
>   $P(D) = 0.000025 \rightarrow$ Probability of a person having the disease <br>
>   $P(D_{c}) = 1- P(D) = 0.999975$ \rightarrow Probability of a person not having the disease <br>
> 
> We'll find the probability of testing positive, given a person doesn't have the disease, i.e.
>   $P(T_{p}|D_{c}) = 1 - P(T_{n}|D_{c}) = 1 - 0.9999 = 0.0001$
> 
> Probability of having the disease given that someone tested positive <br>
>   $P(D|T_{p}) = \frac{P(T_{p}|D)*P(D)}{P(T_{p})}$ <br>
>   $P(D|T_{p}) = \frac{P(T_{p}|D)*P(D)}{P(T_{p}|D)*P(D) + P(T_{p}|D_{c})*P(D_{c})}$ <br>
>   $P(D|T_{p}) = \frac{0.993*0.000025}{0.993*0.000025 + 0.0001*0.999975}$ = 0.1989<br>
>   $P(D|T_{p}) = 0.1989 \rightarrow 19.89\%$ <br>
>
> Therefore, the probability of having the disease given that someone tested positive is **0.1989 or 19.89%**. It has significantly increased from the marginal probability of having the disease which was **0.000025 or 0.0025%**

# 2. Wrangling the Billboard Top 100

using [billboard.csv](./Data/billboard.csv)
``` {r 2setup, include=FALSE}
library(dplyr)
library(tidyverse)

options(dplyr.summarise.inform = FALSE)
```

```{r 2pre}
billboard = read.csv("./Data/billboard.csv")
billboard = billboard[,c("performer", "song", "year", "week", "week_position")]
# Considering the relevant columns as mentioned in the question
head(billboard)
```

## Part. A

Make a table of the top 10 most popular songs since 1958, as measured by the _total number of weeks that a song spent on the Billboard Top 100._  Note that these data end in week 22 of 2021, so the most popular songs of 2021 will not have up-to-the-minute data; please send our apologies to The Weeknd.    

Your table should have __10 rows__ and __3 columns__: `performer`, `song`, and `count`, where `count` represents the number of weeks that song appeared in the Billboard Top 100.  Make sure the entries are sorted in descending order of the `count` variable, so that the more popular songs appear at the top of the table.  Give your table a short caption describing what is shown in the table.  

(_Note_: you'll want to use both `performer` and `song` in any `group_by` operations, to account for the fact that multiple unique songs can share the same title.)  

``` {r 2a, warnings=FALSE}
top_songs_by_week = billboard %>% group_by(song, performer) %>% summarize(count = n())
top10_songs_by_week = head(top_songs_by_week[order(-top_songs_by_week$count),], 10)

knitr::kable(top10_songs_by_week,
             caption = "The Top 10 Songs (Jul 1958 - Jun 2021) by the number of weeks on the Billboard")
```

## Part. B

Is the "musical diversity" of the Billboard Top 100 changing over time?  Let's find out.  We'll measure the musical diversity of given year as _the number of unique songs that appeared in the Billboard Top 100 that year._  Make a line graph that plots this measure of musical diversity over the years.  The x axis should show the year, while the y axis should show the number of unique songs appearing at any position on the Billboard Top 100 chart in any week that year.  For this part, please filter the data set so that it excludes the years 1958 and 2021, since we do not have complete data on either of those years.   Give the figure an informative caption in which you explain what is shown in the figure and comment on any interesting trends you see.

There are number of ways to accomplish the data wrangling here.  We offer you two hints on two possibilities:  

1) You could use two distinct sets of data-wrangling steps.  The first set of steps would get you a table that counts the number of times that a given song appears on the Top 100 in a given year.  The second set of steps operate on the result of the first set of steps; it would count the number of unique songs that appeared on the Top 100 in each year, _irrespective of how many times_ it had appeared.
2) You could use a single set of data-wrangling steps that combines the `length` and `unique` commands. 

``` {r 2b, warnings=FALSE}
musical_diversity = billboard[,c("performer", "song", "year")] 
musical_diversity = musical_diversity %>% filter(!(year %in% c(2021, 1958)))
musical_diversity = unique(musical_diversity)
musical_diversity = musical_diversity %>% group_by(year) %>% summarize(unique=n())
musical_diversity = musical_diversity[order(musical_diversity$year),]

# plot(musical_diversity$year, musical_diversity$unique, type="l", col="red", xlab="Year", ylab="# of unique songs on the Billboard Top-100", main="Musical Diversity")

ggplot(data=musical_diversity, aes(x=year, y=unique, group=1)) +
  geom_line(color="red", size=1) +
  ggtitle("Musical Diversity") +
  xlab("Year") +
  ylab("# of unique songs on the Billboard Top-100")
```

## Part. C

Let's define a "ten-week hit" as a single song that appeared on the Billboard Top 100 for at least ten weeks.  There are 19 artists in U.S. musical history since 1958 who have had _at least 30 songs_ that were "ten-week hits."  Make a bar plot for these 19 artists, showing how many ten-week hits each one had in their musical career.   Give the plot an informative caption in which you explain what is shown.


_Notes_:  

1) You might find this easier to accomplish in two distinct sets of data wrangling steps.
2) Make sure that the individuals names of the artists are readable in your plot, and that they're not all jumbled together.  If you find that your plot isn't readable with vertical bars, you can add a `coord_flip()` layer to your plot to make the bars (and labels) run horizontally instead.
3) By default a bar plot will order the artists in alphabetical order.  This is acceptable to turn in.  But if you'd like to order them according to some other variable, you can use the `fct_reorder` function, described in [this blog post](https://datavizpyr.com/re-ordering-bars-in-barplot-in-r/).  This is optional.

``` {r 2c}
ten_week_hitters = top_songs_by_week %>%
  filter(count >= 10) %>%
  group_by(performer) %>%
  summarize(count = n()) %>%
  filter(count >= 30)

ggplot(data=ten_week_hitters, aes(x=reorder(performer, +count), y=count)) +
  geom_bar(stat="identity", fill="steelblue") +
  ylab("# of 10-week-hits") + xlab("Artist") +
  coord_flip()
```


# 3. Visual story telling part 1: Green Buildings

using [greenbuildings.csv](./Data/greenbuildings.csv) <br>
*\*use raw preview (on the Solutions.Rmd) to view the R-code*

``` {r 3, include=FALSE}
library(dplyr)
library(tidyverse)

green_db = read.csv("./Data/greenbuildings.csv")

green_buildings = green_db[green_db$green_rating == 1,]
non_green_buildings = green_db[green_db$green_rating == 0,]
```

``` {r 3a, echo=FALSE}
cat(dim(green_db[green_db$leasing_rate < 0.1, ])[1], "outliers were removed from the data")
```
160 is a significant number of outliers to be discarded with a justification of data cleaning. Let us also see how the outliers modified the median value of the rent
``` {r 3b, echo=FALSE}
cat(
  "Median rent of all green buildings:", median(green_db[green_db$green_rating == 1, "Rent"]),
  "\nMedian rent of all green buildings after removing the outliers:", median(green_db[green_db$green_rating == 1 & green_db$leasing_rate > 0.1, "Rent"])
  )
```
The outlier removal does not seem to have affected the median rate at all, rendering the process completely redundant. Additionally, removing the outliers might have actually cost us a lot in terms of information loss. Let's see how
``` {r 3c, echo=FALSE}
obs_per_cluster = green_db %>% group_by(cluster) %>% summarise(count=n())
removed_per_cluster = green_db[green_db$leasing_rate < 0.1, ] %>% group_by(cluster) %>% summarise(count=n())

merged = merge(obs_per_cluster, removed_per_cluster, by="cluster", suffixes = c(".total",".removed"))
merged$perc_removed = 100*merged$count.removed/merged$count.total
merged = merged[order(-merged$perc_removed),]

plot_merge = merged[merged$perc_removed >= 20, c("cluster", "perc_removed")]
rownames(plot_merge) = NULL
knitr::kable(plot_merge,
             align = "ll",
             col.names = c("cluster", "perc_removed"),
             caption = "The Percentage of cluster rows lost because of the outlier removal (top 27)")

cat("In these 27 clusters, at least 20% of their properties (rows) were removed when the outliers were discarded. From intuition, we may suppose that each cluster might have characteristic rent of its own. So discarding a significant proportion of each cluster is removing vital information. We can also see that the outlier removal removed ",
      dim(green_db[green_db$leasing_rate < 0.1 & green_db$green_rating == 1, ])[1],
     " of the green rated property.")

```

Let's see how the data is cluster-wise. Let us group the clusters and look at the increase in rent for that cluster for a green building, and then summarize it.

``` {r 3d, echo=FALSE, warning=FALSE}
non_green_medians = non_green_buildings %>% group_by(cluster) %>% summarize(median_rent = median(Rent))
merged = merge(green_db, non_green_medians, by="cluster", suffixes=c(".all", ".non_green"))
comparison = merged[merged$green_rating == 1, c("cluster", "Rent","median_rent")]
comparison$difference = comparison$Rent - comparison$median_rent

cat("Median increase in rent for a green property (among all cluster-wise increases): $", median(comparison$difference), sep=)

ggplot(comparison, aes(x=difference)) + 
  geom_histogram(binwidth=3, color="darkblue", fill="lightblue") +
  geom_vline(xintercept = median(comparison$difference), linetype="dashed",color = "red", size=1) +
  geom_text(aes(x=-3+median(comparison$difference), label="Median Rent Increase", y=35), colour="red", angle=90, text=element_text(size=11)) +
  xlab("Rent difference for green buildings") + ylab("Number of clusers")
most_sign_differences = data.frame(table(cut(comparison$difference, 30)))
most_sign_differences = most_sign_differences[order(-most_sign_differences$Freq),]
rownames(most_sign_differences) = NULL
knitr::kable(
  head(most_sign_differences, 3))
```

From the histogram, we can see that the top 3 most frequent buckets (range of rent increases) are as shown in the table above. Among the top-3, 2 are negative, implying the rents for green buildings are lower than the non-green buildings in some of the clusters. That's all aggregated data. Now let's see how the spread of the rent difference is among clusters.

``` {r 3e, echo=FALSE, warning=FALSE}
iqr = quantile(comparison$difference,probs=c(0.25,0.75))
knitr::kable(iqr)
```

the mid-50% of the data is in the range of (-0.4125,6.3475) meaning we can expect our rent to increase by this amount (per sqaure feet) if we ignore all other factors that might effect the rent. What other factors might be having an effect on the rent?

``` {r 3f, echo=FALSE, warning=FALSE, fig.width=8, fig.height=8}
par(mfrow=c(2,2))
plot(green_db$leasing_rate, green_db$Rent, ylab="Rent", xlab="Leasing Rate")
plot(green_db$Electricity_Costs, green_db$Rent, ylab="Rent", xlab="Electricity cost")
plot(green_db$cd_total_07, green_db$Rent, ylab="Rent", xlab="Cooling  Demand")
plot(green_db$hd_total07, green_db$Rent, ylab="Rent", xlab="Heating  Demand")
```

We can see, although not clearly, that the rent varies with leasing rate, electricity cost, cooling demand, heating demand, etc. all of which depend on the location of the property. This information about our propery under consideration being on East Cesar Chavez, just across I-35 from downtown is never being considered in the analysis. Considering these variations might also give us a more *reliable* prediction of the rent difference.

One more factor considered in the analysis was 100% or 90% occupancy from day-1 of the property being ready for occupancy. This is rarely the case. Let us assume the median rent difference of $3 per square fit per year. and let us consider the property being occupied 100% over a span of some years

``` {r 3g, echo=FALSE}
y3 = c(.33, .66, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)
y5 = c(.2, .4, .6, .8, 1.0, 1.0, 1.0, 1.0)
y8 = c(.125, .25, .375, .5, .625, .75, .875, 1.0)
annual_income_full_occupancy = 250000*3

year3 = c()
year5 = c()
year8 = c()

for(i in 1:8){
  year3 = c(year3, annual_income_full_occupancy*y3[i])
  year5 = c(year5, annual_income_full_occupancy*y5[i])
  year8 = c(year8, annual_income_full_occupancy*y8[i])
}


target_occupancy_100 = c(
  round(3 + ((5000000-sum(year3[1:3]))/annual_income_full_occupancy), 2),
  round(5 + ((5000000-sum(year5[1:5]))/annual_income_full_occupancy), 2),
  round(8 + ((5000000-sum(year8[1:8]))/annual_income_full_occupancy), 2)
)

y3 = y3*0.75
y5 = y3*0.75
y8 = y3*0.75

year3 = c()
year5 = c()
year8 = c()

for(i in 1:8){
  year3 = c(year3, annual_income_full_occupancy*y3[i])
  year5 = c(year5, annual_income_full_occupancy*y5[i])
  year8 = c(year8, annual_income_full_occupancy*y8[i])
}


target_occupancy_75 = c(
  round(3 + ((5000000-sum(year3[1:3]))/(annual_income_full_occupancy*0.75)), 2),
  round(5 + ((5000000-sum(year5[1:5]))/(annual_income_full_occupancy*0.75)), 2),
  round(8 + ((5000000-sum(year8[1:8]))/(annual_income_full_occupancy*0.75)), 2)
)

df = data.frame(target_occupancy_100, target_occupancy_75)
rownames(df) = c("Target occupancy reached by year 3", "Target occupancy reached by year 5", "Target occupancy reached by year 8")
knitr::kable(df,
             align = "ll",
             caption = "The Number of years required to recuperate the premium for green certification")

```

With this analysis, 7.7 years is still the best case scenario of recuperating the premium spent on a green certification. For a more realistic scenario, we can see that the cost will be recuperated before making any profits after at least 10 years of the construction. This gives a more realistic picture by just considering the rent increase of the median cluster disregarding all other factors. We can still suggest the developer to go for a linear regression model to get a more accurate prediction of the returns. Additionally, the linear model would also give a sense the role a particular factor plays in the estimation of the returns.


# 4. Visual story telling part 2: Capital Metro data

using [capmetro_UT.csv](./Data/capmetro_UT.csv) <br>
*\*use raw preview (on the Solutions.Rmd) to view the R-code*

```{r 4a, include=FALSE}
library(lubridate)
library(reshape2)
library(zoo)
```

``` {r 4b, echo=FALSE}
capmetro_df = read.csv("./Data/capmetro_UT.csv")

capmetro_df = capmetro_df %>% mutate(
  timestamp = ymd_hms(timestamp),
  date = date(timestamp),
  year = year(timestamp),
  month = month(timestamp),
  day = day(timestamp),
  hour = hour(timestamp),
  minute = minute(timestamp),
  day_of_week = wday(timestamp, label=TRUE)
)

capmetro_df$traffic = capmetro_df$alighting + capmetro_df$boarding

cols = c("year", "month", "day", "hour", "minute", "date", "day_of_week", "weekend", "temperature", "boarding", "alighting", "traffic")
capmetro_df = capmetro_df[, cols]

knitr::kable(head(capmetro_df), caption="Initial data of CapMetro")
```

We have formatted the initial data, where the date and time are now different columns which we can analyze in terms of each day, month, hour, etc. We have also added a new column called traffic, which is the sum of boarding and alighting signifying the foot traffic in the buses in general.

``` {r 4c, echo=FALSE}
weekly = capmetro_df %>% group_by(day_of_week) %>% summarize(traffic = mean(traffic), boarding = mean(boarding), alighting = mean(alighting))

ggplot(weekly, aes(x=day_of_week, y=traffic)) +
  geom_bar(stat = "identity") + 
  labs(
    title = "Average traffic over the week",
    x = "Day of the week",
    y = "Avg number of people boarding/alighting"
    # subtitle = waiver(),
  )
```

From the overall data, we can clearly see that weekdays have significant traffic compared to weekends.

``` {r 4d, echo=FALSE}
daily = capmetro_df %>% group_by(hour, weekend) %>% summarize(traffic = mean(traffic), boarding = mean(boarding), alighting = mean(alighting))

df = melt(daily,  id = c('hour','weekend'), measure = c('traffic', 'boarding', 'alighting'))

ggplot(df, aes(hour, value, colour = variable)) +
  geom_line(size = 1) +
  facet_grid(cols = vars(weekend))
```

On a particular weekday, we can also see that there are two significant time periods where the traffic is high. Around 8-10 am in the mornings we see that most of the traffic is contributed by the "alighting" passengers meaning buses have more people getting off near the university in the morning. The other peak is around 3-5 pm when most of the students finish their classes, and thus a large percentage of the traffic is contributed by people "boarding" the buses near the university.

We can also see this traffic is significantly high compared to weekends from the second graph.

Let's now see how the traffic varies through the months (Sep 2018 - Nov 2018)

``` {r 4e, echo=FALSE}
monthly = capmetro_df %>% filter(weekend == "weekday") %>% group_by(month) %>% summarize(traffic = mean(traffic), boarding = mean(boarding), alighting = mean(alighting))


ggplot(monthly, aes(x=month, y=traffic)) +
  geom_bar(stat = "identity") + 
  labs(
    title = "Average traffic over the months on weekdays",
    x = "Month",
    y = "Avg number of people boarding/alighting"
    # subtitle = waiver(),
  )
```

We don't see any drastic changes in the three months for which we have the data. How does the traffic change over a closer look at the dates? Does the traffic change over the temperatures of the day? Let's take a look.

``` {r 4g, echo=FALSE}
t = capmetro_df %>% filter(weekend == "weekday") %>% group_by(date) %>% summarize(temp_day = mean(temperature), traffic_day = mean(traffic))

# ggplot(t, aes(date, t)) + 
#   geom_line(size=1, colour = "#4285f4")

df = melt(t,  id = c('date'), measure = c('temp_day', 'traffic_day'))

ggplot(df, aes(date, value, colour = variable)) +
  geom_line(size = 1) +
  facet_grid(rows = vars(variable),scales="free")
```

There seem to be some outliers in the data where the traffic has steeply fallen. Let's see what those outliers are and see if removing them gives us a more clear trend in the data.

``` {r 4h, echo=FALSE}
df = capmetro_df %>% filter(weekend == "weekday") %>% group_by(date) %>% summarize(temp_day = mean(temperature), traffic_day = mean(traffic))

z_scores = as.data.frame(sapply(df[,c("traffic_day")], function(df) (abs(df-mean(df))/sd(df))))
outliers = df[!rowSums(z_scores<3), ]
outliers
```

We can see that these dates where the outliers occurred were university holidays. 3rd September was Labor Day, and the three days in November were the Thanksgiving weekend.

``` {r 4i, echo=FALSE}
no_outliers = df[!rowSums(z_scores>3), ]

df = melt(no_outliers,  id = c('date'), measure = c('temp_day', 'traffic_day'))

ggplot(df, aes(date, value, colour = variable)) +
  geom_line(size = 1) +
  facet_grid(rows = vars(variable),scales="free")
```

We can see an overall decrease in the temperature over the time period of the data from September to November. Along with the decrease in temperature, can we also say there was a decrease in the ridership as it follows a similar trend?

``` {r 4f, echo=FALSE}
summary(capmetro_df)

capmetro_df$bin = cut(capmetro_df$temperature, breaks=15)

ggplot(capmetro_df, aes(x=bin, y=traffic)) +
  geom_bar(stat = "identity") + 
  labs(
    title = "Average traffic over the week",
    x = "Day of the week",
    y = "Avg number of people boarding/alighting"
    # subtitle = waiver(),
  ) + 
  coord_flip()
```

We can see that people prefer to use public transportation (buses) when the weather is good. As the temperature increases from 29 to 75 degrees, we see a drastic increase in ridership. But at the same time, the ridership also decreases drastically when the temperature goes above 80 degrees, reaching almost zero traffic at high temperatures of 93+ degrees.

# Portfolio Modeling

For this question, we decided to do 3 different tests with different types of ETFs to understand the different gains and losses through bootstrapping. 

We first used Oil and Gas commodity ETFs to understand how much we would potentially be gaining or losing if we were to invest in them. 

```{r, include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)

#### Now use a bootstrap approach
#### With more stocks

#Commodities

#USO: https://etfdb.com/etf/USO/
#UNG: https://etfdb.com/etf/UNG/
#OILK: https://etfdb.com/etf/OILK/
#GAZ: https://etfdb.com/etf/GAZ/
#RJN: https://etfdb.com/etf/RJN/

mystocks = c("USO", "UNG", "OILK", "GAZ", "RJN")
myprices = getSymbols(mystocks, from = "2017-08-13")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind( ClCl(USOa),
                     ClCl(UNGa),
                     ClCl(OILKa),
                     ClCl(GAZa),
                     ClCl(RJNa))
```

These are the stocks we used for this portfolio: 

USO: United States Oil Fund LP

UNG: United States Natural Gas Fund LP

OILK: ProShares K-1 Free Crude Oil Strategy ETF

GAZ: iPath Series B Bloomberg Natural Gas Subindex Total Return ETN

RJN: Elements Rogers International Commodity Index-Energy Total Return ETN


```{r, echo = FALSE}
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```

From the coefficients we can see the difference in each stocks and how they differ from day to day. We started measuring from 5 years back till today. 

We then made a pairs plot to see each ETF against each other

```{r, echo = FALSE}
# Compute the returns from the closing prices
pairs(all_returns)
```

We then simulate a random day to where we distribute each ETF to our $100K capital equally and then find the sum to compute our new total wealth at the end of the random day. 

```{r, echo = FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.2,0.2,0.2, 0.2, 0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
holdings
total_wealth = sum(holdings)
total_wealth
```

So as we can see on a random day we went from $100,000 in capital to the output.

We then observed the change over a time period of two weeks. 

```{r, echo = FALSE}
# Now loop over two trading weeks
# let's run the following block of code 5 or 6 times
# to eyeball the variability in performance trajectories

## begin block
total_wealth = 100000
weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')
## end block
```

So we can see the change in our capital over the time of 20 days.

So now we bootstrap this model.

```{r, echo = FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

So we can see the distribution of capital after bootstrapping. It is mostly normal.

```{r, echo = FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

# note: this is  a negative number (a loss, e.g. -500), but we conventionally
# express VaR as a positive number (e.g. 500)
```

So this is our ultimate gain/loss, which can vary from time to time. 


Our next portfolio is emerging market equities, which can be important for people who are interested in investing in new markets which can be relatively risky.

```{r, include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)

#### Now use a bootstrap approach
#### With more stocks

#Commodities

#VTO: https://etfdb.com/etf/VTO/
#IEMG: https://etfdb.com/etf/IEMG/
#GEM: https://etfdb.com/etf/GEM/
#DEM: https://etfdb.com/etf/DEM/
#PXH: https://etfdb.com/etf/PXH/

mystocks = c("VTO", "IEMG", "GEM", "DEM", "PXH")
myprices = getSymbols(mystocks, from = "2017-08-13")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind( ClCl(VTOa),
                     ClCl(IEMGa),
                     ClCl(GEMa),
                     ClCl(DEMa),
                     ClCl(PXHa))
```

These are the stocks we used for this portfolio: 

VTO: Vanguard FTSE Emerging Markets ETF

IEMG: iShares Core MSCI Emerging Markets ETF

GEM: Goldman Sachs ActiveBeta Emerging Markets Equity ETF

DEM: WisdomTree Emerging Markets High Dividend Fund

PXH: Invesco FTSE RAFI Emerging Markets ETF


```{r, echo = FALSE}
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```

From the coefficients we can see the difference in each stocks and how they differ from day to day. We started measuring from 5 years back till today. 

We then made a pairs plot to see each ETF against each other

```{r, echo = FALSE}
# Compute the returns from the closing prices
pairs(all_returns)
```

We then simulate a random day to where we distribute each ETF to our $100K capital equally and then find the sum to compute our new total wealth at the end of the random day. 

```{r, echo = FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.2,0.2,0.2, 0.2, 0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
holdings
total_wealth = sum(holdings)
total_wealth
```

So as we can see on a random day we went from $100,000 in capital to output [1]. This amount varies from day to day.

We then observed the change over a time period of two weeks. 

```{r, echo = FALSE}
# Now loop over two trading weeks
# let's run the following block of code 5 or 6 times
# to eyeball the variability in performance trajectories

## begin block
total_wealth = 100000
weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')
## end block
```

So we can see the change in our capital over the time of 20 days.

So now we bootstrap this model.

```{r, echo = FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

So we can see the distribution of capital after bootstrapping. It is mostly normal. 

```{r, echo = FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

# note: this is  a negative number (a loss, e.g. -500), but we conventionally
# express VaR as a positive number (e.g. 500)
```

So our ultimate gain/loss would be output [1]. This could vary due to randomization in bootstrapping. 

The last portfolio we used is with Health and Biotech Equities, which is slightly less common but still has a large capital. 

```{r, include = FALSE}
library(mosaic)
library(quantmod)
library(foreach)

#### Now use a bootstrap approach
#### With more stocks

#Commodities

#XHE: https://etfdb.com/etf/XHE/
#PSCH: https://etfdb.com/etf/PSCH/
#IHE: https://etfdb.com/etf/IHE/
#PBE: https://etfdb.com/etf/PBE/
#FHLC: https://etfdb.com/etf/FHLC/

mystocks = c("XHE", "PSCH", "IHE", "PBE", "FHLC")
myprices = getSymbols(mystocks, from = "2017-08-13")

# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

head(USOa)

# Combine all the returns in a matrix
all_returns = cbind( ClCl(XHEa),
                     ClCl(PSCHa),
                     ClCl(IHEa),
                     ClCl(PBEa),
                     ClCl(FHLCa))
```

These are the stocks we used for this portfolio: 

XHE: SPDR S&P Health Care Equipment ETF

PSCH: Invesco S&P SmallCap Health Care ETF

IHE: IShares U.S. Pharmaceuticals ETF

PBE: Invesco Dynamic Biotechnology & Genome ETF

FHLC: Fidelity MSCI Health Care Index ETF


```{r, echo = FALSE}
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```

From the coefficients we can see the difference in each stocks and how they differ from day to day. We started measuring from 5 years back till today. 

We then made a pairs plot to see each ETF against each other

```{r, echo = FALSE}
# Compute the returns from the closing prices
pairs(all_returns)
```

We then simulate a random day to where we distribute each ETF to our $100K capital equally and then find the sum to compute our new total wealth at the end of the random day. 

```{r, echo = FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.2,0.2,0.2, 0.2, 0.2)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)

# Compute your new total wealth
holdings
total_wealth = sum(holdings)
total_wealth
```

So as we can see on a random day we went from $100,000 in capital to output[1]. 

We then observed the change over a time period of two weeks. 

```{r, echo = FALSE}
# Now loop over two trading weeks
# let's run the following block of code 5 or 6 times
# to eyeball the variability in performance trajectories

## begin block
total_wealth = 100000
weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings = weights * total_wealth
n_days = 20  # capital T in the notes
wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
  return.today = resample(all_returns, 1, orig.ids=FALSE)  # sampling from R matrix in notes
  holdings = holdings + holdings*return.today
  total_wealth = sum(holdings)
  wealthtracker[today] = total_wealth
}
total_wealth
plot(wealthtracker, type='l')
## end block
```

So we can see the change in our capital over the time of 20 days.

So now we bootstrap this model.

```{r, echo = FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25)
```

So we can see the distribution of capital after bootstrapping. It is mostly normal. 

```{r, echo = FALSE}
# Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
hist(sim1[,n_days]- initial_wealth, breaks=30)

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)

# note: this is  a negative number (a loss, e.g. -500), but we conventionally
# express VaR as a positive number (e.g. 500)
```

So our ultimate gain/loss would be output [1], however this could change due to randomization in bootstrapping.

When we compare all three, we can see which portfolio is better to invest in, as it gains more capital or loses the least. 

# Market Segmentation

The social marketing data set consists of users and their different tweets categorized into 35 main topics. Our goal was to define a "market segment" using this data set. We initially took a look at the data and decided which type of analysis was to fit best to define the market segment. 

Since there were likely to be groups of different types of tweets, we decided to take a look at hierarchical clustering, however this did not end with favorable results, as there are too many splits on the decision tree and whichever k we chose was leading to an uneven distribution. Even with a K++ cluster we ended up with very high numbers, and decided to move to the next type of analysis. 

```{r, include = FALSE}
library(tidyverse)
library(ggplot2)
library(ggcorrplot)

social_marketing <- read.csv("C:/Users/tanvi/Downloads/social_marketing.csv", row.names = 1)

social_marketing <- cbind(User = rownames(social_marketing), social_marketing)
rownames(social_marketing) <- 1:nrow(social_marketing)

social_results = social_marketing %>%
  group_by(User) %>% 
  select(-User) %>%
  summarize_all(mean) %>%
  column_to_rownames(var="User")
```

We then attempted PCA to understand through which factors leads in the most variance of the data set. This split could potentially be defined as the market segment, so we decided to take a look into that. 

We ran a quick few plots to see the relationship between the User and various categories. In the following graphs we can see the relationship between chatter tweets by the user and travel tweets by the user respectively.

```{r, echo = FALSE}
ggplot(rownames_to_column(social_results, "User")) + 
  geom_col(aes(x=reorder(User, -chatter), y = chatter)) + 
  coord_flip()

ggplot(rownames_to_column(social_results, "User")) + 
  geom_col(aes(x=reorder(User, -travel), y = travel)) + 
  coord_flip()
```

We can see that there is some sort of relationship, so we proceeded with PCA.

We took a more general look at the correlation between each variable in the following graph.

```{r, echo = FALSE}
# a look at the correlation matrix
#cor(social_results)

# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(social_results), hc.order = TRUE)
```

Looking through the correlation plot, we can see some variables that have a fairly strong correlation with each other.

Next we took a look at the distribution of variance among the data set. This will then split our PCs so we can observe them individually. 

```{r, echo = FALSE}
# Now look at PCA of the (average) survey responses.  
PCApilot = prcomp(social_results, scale=TRUE)

## variance plot
plot(PCApilot)
summary(PCApilot)
```

We can see that around PC16 gives us around 75% of the cumulative proportion, which is around 1/2 of the PCs. So we decided to take a look at a few.

The following table shows the first 10 PCs. We decided to take a look at the first three to see if we can draw a conclusion.

```{r, echo = FALSE}
# first few pcs
# try interpreting the loadings
# the question to ask is: "which variables does this load heavily on (positive and negatively)?"
round(PCApilot$rotation[,1:10],2) 

# create a tidy summary of the loadings
loadings_summary = PCApilot$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Question')
```

```{r, echo = FALSE}
# not sure yet
loadings_summary %>%
  select(Question, PC1) %>%
  arrange(desc(PC1))
```

After looking at PC1, we did not find any obvious meaning of the vectors, meaning that it is likely noise, so we proceeded to the next PC.

```{r, echo = FALSE}
#  this looks like influencer tweets vs serious tweets
loadings_summary %>%
  select(Question, PC2) %>%
  arrange(desc(PC2))
```

Looking at the categories for the tweets and how they are arranged, we can see that there is a separation between recreational tweets such as cooking, shopping, fashion, and serious tweets such as politics and news.

Now lets look at PC3.

```{r, echo = FALSE}
loadings_summary %>%
  select(Question, PC3) %>%
  arrange(desc(PC3))
```

PC3 looks similar to PC2, except that some of the variables have different vectors. We can conclude that this is also similar to recreational or influencer tweets vs serious tweets.

We then made some plots using the PCs that worked. 

```{r, include = FALSE}
# Let's make some plots of the shows themselves in 
# PC space, i.e. the space of summary variables we've created

social_marketing <- read.csv("./data/social_marketing.csv", row.names = 1)


social_marketing = merge(social_marketing, PCApilot$x[,1:3], by="row.names")
social_marketing = rename(social_marketing, social_marketing = Row.names)
```


```{r, echo = FALSE}

# let's plot in PC1 space
# We might feel good calling PC1 the "quality drama" PC
ggplot(social_marketing) + 
  geom_col(aes(x=reorder(social_marketing, PC2), y=PC2)) + 
  coord_flip()

```

```{r, echo = FALSE}
# looks like a "lighthearted vs serious" PC
ggplot(social_marketing) + 
  geom_col(aes(x=reorder(social_marketing, PC3), y=PC3)) + 
  coord_flip()
```

Both of these graphs look similar but have a distinct distribution

```{r, echo = FALSE}
# principal component regression: predicted engagement
lm1 = lm(travel ~ PC1 + PC2 + PC3, data=social_marketing)
summary(lm1)

# gross ratings points
lm2 = lm(college_uni ~ PC1 + PC2 + PC3, data=social_marketing)
summary(lm2)

# Conclusion: we can predict engagement and ratings
# with PCA summaries of the pilot survey.
# probably too much variance to regress on all survey questions!
# since the sample size isn't too large here.
plot(travel ~ fitted(lm1), data=social_marketing)
plot(college_uni ~ fitted(lm2), data=social_marketing)
```
After plotting linear models with all 3 PCs, we can see that the main difference in tweets is recreational vs serious. We can use this to define our market segment. In this dataset, the market is separated by the types of tweets that are recreational and serious. It shows the main differences in tweets by users. 
